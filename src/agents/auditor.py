# """
# Agent Auditeur (The Auditor)
# RÃ´le : Analyser le code Python, dÃ©tecter les problÃ¨mes de qualitÃ© et gÃ©nÃ©rer un rapport.
# """

# import os
# from dotenv import load_dotenv
# from typing import Dict, List
# from langchain_google_genai import ChatGoogleGenerativeAI
# from src.utils.logger import log_experiment, ActionType

# load_dotenv()

# from src.tools.file_tools import read_file_safe, list_python_files
# from src.tools.pylint_tool import run_pylint, parse_pylint_output

# try:
#     from src.prompts.auditor_prompts import AUDITOR_SYSTEM_PROMPT
# except ImportError:
#     AUDITOR_SYSTEM_PROMPT = """Tu es un expert Python chargÃ© d'analyser du code.

# MISSION :
# Analyse le code fourni et les rapports pylint, puis gÃ©nÃ¨re un rapport JSON.

# FORMAT DE SORTIE (STRICT) :
# {
#     "files_analyzed": ["file1.py", "file2.py"],
#     "total_issues": 5,
#     "issues": [
#         {
#             "file": "example.py",
#             "line": 10,
#             "severity": "high",
#             "type": "missing_docstring",
#             "message": "La fonction manque de docstring"
#         }
#     ],
#     "recommendations": ["Ajouter des docstrings", "Corriger la syntaxe"]
# }

# RÃ©ponds UNIQUEMENT avec du JSON valide, sans texte avant ou aprÃ¨s."""


# class AuditorAgent:
#     """
#     Agent responsable de l'audit du code.
#     Analyse la qualitÃ©, dÃ©tecte les bugs et gÃ©nÃ¨re un rapport.
#     """

#     def __init__(self, model_name: str = "gemini-2.5-flash-lite"):
#         self.model_name = model_name
#         self.llm = ChatGoogleGenerativeAI(
#             model=model_name,
#             temperature=0.1,
#             convert_system_message_to_human=True
#         )
#         print(f"AuditorAgent initialisÃ© avec le modÃ¨le : {model_name}")

#     def analyze(self, target_dir: str) -> Dict:
#         """
#         Analyse tous les fichiers Python d'un dossier.

#         Args:
#             target_dir: Chemin du dossier Ã  analyser

#         Returns:
#             Dict: Rapport d'audit contenant les problÃ¨mes dÃ©tectÃ©s
#         """
#         print(f"\n [AUDITOR] DÃ©marrage de l'analyse de : {target_dir}")

#         try:
#             print(" Recherche des fichiers Python...")
#             python_files = list_python_files(target_dir)

#             if not python_files:
#                 print("  Aucun fichier Python trouvÃ© dans le dossier.")
#                 return {
#                     "files_analyzed": [],
#                     "total_issues": 0,
#                     "issues": [],
#                     "recommendations": ["Aucun fichier Python Ã  analyser"]
#                 }

#             print(f" {len(python_files)} fichier(s) Python trouvÃ©(s)")

#             all_issues = []
#             files_analyzed = []

#             # âœ… Boucle sur chaque fichier
#             for filename in python_files:
#                 print(f"\n Analyse de : {filename}")

#                 full_path = os.path.join(target_dir, filename)
#                 file_content = read_file_safe(full_path, target_dir)
#                 pylint_output = run_pylint(full_path)
#                 pylint_result = parse_pylint_output(pylint_output)

#                 # Construire le prompt pour le LLM
#                 user_prompt = self._build_analysis_prompt(
#                     filename=filename,
#                     file_content=file_content,
#                     pylint_result=pylint_result
#                 )

#                 print(f" Consultation du LLM pour l'analyse...")
#                 llm_response = self._call_llm(user_prompt)

#                 # Extraire les problÃ¨mes
#                 file_issues = self._parse_llm_response(llm_response, filename)
#                 all_issues.extend(file_issues)
#                 files_analyzed.append(filename)
                
#                 # Logger le rÃ©sultat pour ce fichier
#                 log_experiment(
#                     agent_name="Auditor_Agent",
#                     model_used=self.model_name,
#                     action=ActionType.ANALYSIS,
#                     details={
#                         "file_analyzed": filename,
#                         #"pylint_score": pylint_result.get("score", 0),
#                         "input_prompt": user_prompt,
#                         "output_response": llm_response,
#                         "issues_found": len(file_issues),
#                     },
#                     status="SUCCESS"
#                 )

                

#                 # Afficher le nombre de problÃ¨mes pour ce fichier
#                 print(f" Analyse terminÃ©e pour {filename} : {len(file_issues)} problÃ¨me(s) dÃ©tectÃ©(s)")

#             # Rapport final
#             report = {
#                 "files_analyzed": files_analyzed,
#                 "total_issues": len(all_issues),
#                 "issues": all_issues,
#                 "recommendations": self._generate_recommendations(all_issues)
#             }

#             print(f"\n [AUDITOR] Analyse terminÃ©e : {report['total_issues']} problÃ¨me(s) au total")
#             return report

#         except Exception as e:
#             print(f" [AUDITOR] Erreur lors de l'analyse : {str(e)}")
#             log_experiment(
#                 agent_name="Auditor_Agent",
#                 model_used=self.model_name,
#                 action=ActionType.DEBUG,
#                 details={
#                     "input_prompt": f"Analyse du dossier {target_dir}",
#                     "output_response": f"Erreur : {str(e)}",
#                     "error_type": type(e).__name__
#                 },
#                 status="FAILURE"
#             )
#             raise

#     def _build_analysis_prompt(self, filename: str, file_content: str, pylint_result: Dict) -> str:
#         pylint_issues = pylint_result.get('issues', [])
#         issues_summary = pylint_issues[:10] if len(pylint_issues) > 10 else pylint_issues

#         return f"""Analyse ce code Python et le rapport pylint.

# FICHIER : {filename}
# SCORE PYLINT : {pylint_result.get('score', 0)}/10

# # CODE :
# # ```python
# # {file_content}
# # ```

# # ERREURS PYLINT (Ã©chantillon) :
# # {issues_summary}

# # GÃ©nÃ¨re un rapport JSON avec les problÃ¨mes dÃ©tectÃ©s les plus importants."""
    
#     def _call_llm(self, prompt: str) -> str:
#         """
#         Appelle le LLM avec le prompt systÃ¨me et le prompt utilisateur.
        
#         Args:
#             prompt: Prompt utilisateur
            
#         Returns:
#             str: RÃ©ponse du LLM
#         """
#         messages = [
#             {"role": "system", "content": AUDITOR_SYSTEM_PROMPT},
#             {"role": "user", "content": prompt}
#         ]
        
#         response = self.llm.invoke(messages)
#         return response.content
    
#     def _parse_llm_response(self, response: str, filename: str) -> List[Dict]:
#         """
#         Parse la rÃ©ponse JSON du LLM.
        
#         Args:
#             response: RÃ©ponse brute du LLM
#             filename: Nom du fichier analysÃ©
            
#         Returns:
#             List[Dict]: Liste des problÃ¨mes dÃ©tectÃ©s
#         """
#         import json
        
#         try:
            
#             cleaned = response.strip()
#             if cleaned.startswith("```json"):
#                 cleaned = cleaned[7:]
#             if cleaned.startswith("```"):
#                 cleaned = cleaned[3:]
#             if cleaned.endswith("```"):
#                 cleaned = cleaned[:-3]
#             cleaned = cleaned.strip()
            
#             data = json.loads(cleaned)
#             issues = data.get("issues", [])
            
            
#             for issue in issues:
#                 if "file" not in issue:
#                     issue["file"] = filename
            
#             return issues
            
#         except json.JSONDecodeError as e:
#             print(f"  Impossible de parser la rÃ©ponse JSON du LLM : {str(e)}")
            
#             return [{
#                 "file": filename,
#                 "line": 0,
#                 "severity": "medium",
#                 "type": "parse_error",
#                 "message": "Erreur lors de l'analyse LLM"
#             }]
    
#     def _generate_recommendations(self, issues: List[Dict]) -> List[str]:
#         """
#         GÃ©nÃ¨re des recommandations basÃ©es sur les problÃ¨mes dÃ©tectÃ©s.
        
#         Args:
#             issues: Liste des problÃ¨mes
            
#         Returns:
#             List[str]: Recommandations
#         """
#         if not issues:
#             return ["Le code semble conforme aux standards Python"]
        
#         recommendations = []
#         issue_types = set(issue.get("type", "") for issue in issues)
        
#         if "missing_docstring" in issue_types:
#             recommendations.append("Ajouter des docstrings aux fonctions et classes")
#         if "syntax_error" in issue_types:
#             recommendations.append("Corriger les erreurs de syntaxe")
#         if "naming_convention" in issue_types:
#             recommendations.append("Respecter les conventions de nommage PEP8")
#         if "import_error" in issue_types:
#             recommendations.append("VÃ©rifier les imports")
#         if "unused_variable" in issue_types:
#             recommendations.append("Supprimer les variables non utilisÃ©es")
            
#         return recommendations if recommendations else ["AmÃ©liorer la qualitÃ© gÃ©nÃ©rale du code"]
"""
Agent Auditeur (The Auditor)
RÃ´le : Analyser le code Python, dÃ©tecter les problÃ¨mes de qualitÃ© et gÃ©nÃ©rer un rapport.
"""

import os
from dotenv import load_dotenv 
from typing import Dict, List
from langchain_google_genai import ChatGoogleGenerativeAI
from src.utils.logger import log_experiment, ActionType

load_dotenv()

from src.tools.file_tools import read_file_safe, list_python_files
from src.tools.pylint_tool import run_pylint, parse_pylint_output

try:
    from src.prompts.auditor_prompts import AUDITOR_SYSTEM_PROMPT
except ImportError:
    AUDITOR_SYSTEM_PROMPT = """Tu es un expert Python chargÃ© d'analyser du code.

MISSION :
Analyse le code fourni et les rapports pylint, puis gÃ©nÃ¨re un rapport JSON.

FORMAT DE SORTIE (STRICT) :
{
    "files_analyzed": ["file1.py", "file2.py"],
    "total_issues": 5,
    "issues": [
        {
            "file": "example.py",
            "line": 10,
            "severity": "high",
            "type": "missing_docstring",
            "message": "La fonction manque de docstring"
        }
    ],
    "recommendations": ["Ajouter des docstrings", "Corriger la syntaxe"]
}

RÃ©ponds UNIQUEMENT avec du JSON valide, sans texte avant ou aprÃ¨s."""


class AuditorAgent:
    """
    Agent responsable de l'audit du code.
    Analyse la qualitÃ©, dÃ©tecte les bugs et gÃ©nÃ¨re un rapport.
    
    Correspond Ã  l'Agent Auditeur (The Auditor) du TP "The Refactoring Swarm".
    """
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialise l'agent auditeur.
        
        Args:
            model_name: Nom du modÃ¨le LLM Ã  utiliser (recommandÃ©: gemini-2.0-flash-exp)
        """
        self.model_name = model_name
        
        api_key = os.getenv("GOOGLE_API_KEY")
        
        if not api_key:
            raise ValueError(
                "âŒ ClÃ© API Google non trouvÃ©e. "
                "Assurez-vous d'avoir GOOGLE_API_KEY dans votre fichier .env"
            )
        
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.1,
            convert_system_message_to_human=True
        )
        print(f"âœ… AuditorAgent initialisÃ© avec le modÃ¨le : {model_name}")
    
    def analyze(self, target_dir: str) -> Dict:
        """
        Analyse tous les fichiers Python d'un dossier et logue chaque fichier individuellement.
        
        Cette mÃ©thode respecte le protocole de logging du TP en enregistrant :
        - file_analyzed : Nom du fichier Python analysÃ©
        - issues_found : Nombre d'erreurs dÃ©tectÃ©es pour ce fichier
        - input_prompt : Le prompt envoyÃ© au LLM (OBLIGATOIRE)
        - output_response : La rÃ©ponse du LLM (OBLIGATOIRE)
        
        Args:
            target_dir: Chemin du dossier Ã  analyser (ex: "./sandbox/dataset_inconnu")
            
        Returns:
            Dict: Rapport d'audit contenant les problÃ¨mes dÃ©tectÃ©s
        """
        print(f"\nğŸ” [AUDITOR] DÃ©marrage de l'analyse de : {target_dir}")
        
        try:
            # âœ… Ã‰TAPE 1 : Recherche des fichiers Python
            print("ğŸ“‚ Recherche des fichiers Python...")
            python_files = list_python_files(target_dir)
            
            if not python_files:
                print("âš ï¸  Aucun fichier Python trouvÃ© dans le dossier.")
                
                # Logger l'absence de fichiers
                log_experiment(
                    agent_name="Auditor_Agent",
                    model_used=self.model_name,
                    action=ActionType.ANALYSIS,
                    details={
                        "file_analyzed": "N/A",
                        "input_prompt": f"Analyse du dossier {target_dir}",
                        "output_response": "Aucun fichier Python trouvÃ© dans le dossier cible",
                        "issues_found": 0,
                        "target_directory": target_dir
                    },
                    status="SUCCESS"
                )
                
                return {
                    "files_analyzed": [],
                    "total_issues": 0,
                    "issues": [],
                    "recommendations": ["Aucun fichier Python Ã  analyser"]
                }
            
            print(f"ğŸ“ {len(python_files)} fichier(s) Python trouvÃ©(s)")
            
            # âœ… Ã‰TAPE 2 : Analyse de chaque fichier individuellement
            all_issues = []
            files_analyzed = []
            
            for filename in python_files:
                print(f"\nğŸ“„ Analyse de : {filename}")
                
                try:
                    # Lecture du fichier
                    full_path = os.path.join(target_dir, filename)
                    file_content = read_file_safe(full_path, target_dir)
                    
                    # Analyse Pylint
                    print(f"  âš™ï¸ ExÃ©cution de pylint sur {filename}...")
                    pylint_output = run_pylint(full_path)
                    pylint_result = parse_pylint_output(pylint_output)
                    pylint_score = pylint_result.get("score", 0)
                    pylint_issues = pylint_result.get("issues", [])
                    
                    print(f"  ğŸ“Š Score Pylint : {pylint_score}/10")
                    print(f"  ğŸ” ProblÃ¨mes Pylint dÃ©tectÃ©s : {len(pylint_issues)}")
                    
                    # Construction du prompt pour le LLM
                    user_prompt = self._build_analysis_prompt(
                        filename=filename,
                        file_content=file_content,
                        pylint_result=pylint_result
                    )
                    
                    # Appel du LLM
                    print(f"  ğŸ¤– Consultation du LLM pour l'analyse approfondie...")
                    llm_response = self._call_llm(user_prompt)
                    
                    # Parser la rÃ©ponse du LLM
                    file_issues = self._parse_llm_response(llm_response, filename)
                    file_issues_count = len(file_issues)
                    
                    # âœ… LOGGING OBLIGATOIRE pour chaque fichier
                    log_experiment(
                        agent_name="Auditor_Agent",
                        model_used=self.model_name,
                        action=ActionType.ANALYSIS,
                        details={
                            "file_analyzed": filename,  # âœ… OBLIGATOIRE : Nom du fichier
                            "input_prompt": user_prompt,  # âœ… OBLIGATOIRE : Prompt envoyÃ©
                            "output_response": llm_response,  # âœ… OBLIGATOIRE : RÃ©ponse LLM
                            "issues_found": file_issues_count,  # âœ… OBLIGATOIRE : Nombre d'erreurs
                            "pylint_score": pylint_score,
                            "pylint_issues_count": len(pylint_issues),
                            "file_path": full_path,
                            "code_lines": len(file_content.split('\n')) if file_content else 0
                        },
                        status="SUCCESS"  # âœ… L'analyse a fonctionnÃ© (mÃªme si erreurs dÃ©tectÃ©es)
                    )
                    
                    # Ajouter les problÃ¨mes Ã  la liste globale
                    all_issues.extend(file_issues)
                    files_analyzed.append(filename)
                    
                    print(f"  {'âœ…' if file_issues_count == 0 else 'âŒ'} Analyse terminÃ©e : {file_issues_count} problÃ¨me(s) dÃ©tectÃ©(s)")
                    
                except Exception as e:
                    print(f"âš ï¸ Erreur lors de l'analyse de {filename} : {str(e)}")
                    
                    # Logger l'erreur pour ce fichier spÃ©cifique
                    log_experiment(
                        agent_name="Auditor_Agent",
                        model_used=self.model_name,
                        action=ActionType.DEBUG,
                        details={
                            "file_analyzed": filename,
                            "input_prompt": f"Tentative d'analyse de {filename} dans {target_dir}",
                            "output_response": f"âŒ Erreur : {str(e)}",
                            "issues_found": 1,
                            "error_type": type(e).__name__
                        },
                        status="FAILURE"  # âœ… L'action a Ã©chouÃ© (erreur systÃ¨me)
                    )
                    
                    # Ajouter une erreur gÃ©nÃ©rique
                    all_issues.append({
                        "file": filename,
                        "line": 0,
                        "severity": "high",
                        "type": "analysis_error",
                        "message": f"Erreur lors de l'analyse : {str(e)}"
                    })
            
            # âœ… Ã‰TAPE 3 : GÃ©nÃ©ration du rapport final
            report = {
                "files_analyzed": files_analyzed,
                "total_issues": len(all_issues),
                "issues": all_issues,
                "recommendations": self._generate_recommendations(all_issues)
            }
            
            print(f"\nâœ… [AUDITOR] Analyse terminÃ©e : {report['total_issues']} problÃ¨me(s) au total")
            
            return report
            
        except Exception as e:
            print(f"âŒ [AUDITOR] Erreur critique lors de l'analyse : {str(e)}")
            
            # Logger l'erreur globale
            log_experiment(
                agent_name="Auditor_Agent",
                model_used=self.model_name,
                action=ActionType.DEBUG,
                details={
                    "file_analyzed": "system_error",
                    "input_prompt": f"Analyse du dossier {target_dir}",
                    "output_response": f"âŒ Erreur systÃ¨me : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            raise
    
    def _build_analysis_prompt(self, filename: str, file_content: str, 
                               pylint_result: Dict) -> str:
        """
        Construit le prompt pour l'analyse d'un fichier.
        
        Args:
            filename: Nom du fichier
            file_content: Contenu du fichier
            pylint_result: RÃ©sultat de l'analyse pylint
            
        Returns:
            str: Prompt formatÃ© pour le LLM
        """
        pylint_issues = pylint_result.get('issues', [])
        issues_summary = pylint_issues[:10] if len(pylint_issues) > 10 else pylint_issues
        
        return f"""Analyse ce code Python et le rapport pylint pour identifier les problÃ¨mes de qualitÃ©.

FICHIER : {filename}
SCORE PYLINT : {pylint_result.get('score', 0)}/10
NOMBRE D'ERREURS PYLINT : {len(pylint_issues)}

CODE :
```python
{file_content[:2000]}{"..." if len(file_content) > 2000 else ""}
```

ERREURS PYLINT (Ã©chantillon des plus importantes) :
{issues_summary}

INSTRUCTIONS :
1. Identifie les problÃ¨mes les plus critiques (bugs, erreurs de logique, code non maintenable)
2. Classe-les par sÃ©vÃ©ritÃ© : "high", "medium", "low"
3. Propose des recommandations concrÃ¨tes

GÃ©nÃ¨re un rapport JSON strict avec les champs : files_analyzed, total_issues, issues, recommendations.
RÃ©ponds UNIQUEMENT avec du JSON valide."""
    
    def _call_llm(self, prompt: str) -> str:
        """
        Appelle le LLM avec le prompt systÃ¨me et le prompt utilisateur.
        
        Args:
            prompt: Prompt utilisateur
            
        Returns:
            str: RÃ©ponse du LLM
        """
        messages = [
            {"role": "system", "content": AUDITOR_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = self.llm.invoke(messages)
        
        # Gestion des diffÃ©rents formats de rÃ©ponse
        if isinstance(response.content, list):
            content = response.content[0] if response.content else ""
            if hasattr(content, 'text'):
                return content.text
            return str(content)
        
        return response.content
    
    def _parse_llm_response(self, response: str, filename: str) -> List[Dict]:
        """
        Parse la rÃ©ponse JSON du LLM.
        
        Args:
            response: RÃ©ponse brute du LLM
            filename: Nom du fichier analysÃ©
            
        Returns:
            List[Dict]: Liste des problÃ¨mes dÃ©tectÃ©s
        """
        import json
        
        try:
            # Nettoyer la rÃ©ponse des balises markdown
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            # Parser le JSON
            data = json.loads(cleaned)
            issues = data.get("issues", [])
            
            # S'assurer que chaque problÃ¨me a un champ "file"
            for issue in issues:
                if "file" not in issue:
                    issue["file"] = filename
            
            return issues
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸  Impossible de parser la rÃ©ponse JSON du LLM : {str(e)}")
            print(f"RÃ©ponse brute : {response[:200]}...")
            
            # Retourner une erreur de parsing
            return [{
                "file": filename,
                "line": 0,
                "severity": "medium",
                "type": "parse_error",
                "message": "Erreur lors de l'analyse LLM - RÃ©ponse non valide"
            }]
    
    def _generate_recommendations(self, issues: List[Dict]) -> List[str]:
        """
        GÃ©nÃ¨re des recommandations basÃ©es sur les problÃ¨mes dÃ©tectÃ©s.
        
        Args:
            issues: Liste des problÃ¨mes
            
        Returns:
            List[str]: Recommandations
        """
        if not issues:
            return ["âœ… Le code semble conforme aux standards Python"]
        
        recommendations = []
        issue_types = set(issue.get("type", "") for issue in issues)
        
        # Recommandations basÃ©es sur les types de problÃ¨mes
        if "missing_docstring" in issue_types:
            recommendations.append("ğŸ“ Ajouter des docstrings aux fonctions et classes")
        if "syntax_error" in issue_types:
            recommendations.append("ğŸ”§ Corriger les erreurs de syntaxe")
        if "naming_convention" in issue_types:
            recommendations.append("ğŸ“ Respecter les conventions de nommage PEP8")
        if "import_error" in issue_types:
            recommendations.append("ğŸ“¦ VÃ©rifier et corriger les imports")
        if "unused_variable" in issue_types:
            recommendations.append("ğŸ§¹ Supprimer les variables non utilisÃ©es")
        if "complexity" in issue_types:
            recommendations.append("ğŸ”„ RÃ©duire la complexitÃ© cyclomatique des fonctions")
        if "security" in issue_types:
            recommendations.append("ğŸ”’ Corriger les vulnÃ©rabilitÃ©s de sÃ©curitÃ©")
        if "type_error" in issue_types:
            recommendations.append("ğŸ”¤ Corriger les erreurs de typage")
        if "logic_error" in issue_types:
            recommendations.append("ğŸ§  Corriger les erreurs de logique")
        
        # Recommandations basÃ©es sur la sÃ©vÃ©ritÃ©
        high_severity_count = sum(1 for issue in issues if issue.get("severity") == "high")
        medium_severity_count = sum(1 for issue in issues if issue.get("severity") == "medium")
        low_severity_count = sum(1 for issue in issues if issue.get("severity") == "low")
        
        if high_severity_count > 0:
            recommendations.insert(0, f"âš ï¸ PRIORITÃ‰ : Corriger les {high_severity_count} problÃ¨me(s) de sÃ©vÃ©ritÃ© HAUTE")
        
        if medium_severity_count > 3:
            recommendations.append(f"âš ï¸ Attention : {medium_severity_count} problÃ¨me(s) de sÃ©vÃ©ritÃ© moyenne Ã  traiter")
        
        if low_severity_count > 5:
            recommendations.append(f"â„¹ï¸ {low_severity_count} problÃ¨me(s) mineurs identifiÃ©s")
            
        return recommendations if recommendations else ["ğŸ”§ AmÃ©liorer la qualitÃ© gÃ©nÃ©rale du code"]