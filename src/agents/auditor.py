"""
Agent Auditeur (The Auditor)
R√¥le : Analyser le code Python, d√©tecter les probl√®mes de qualit√© et g√©n√©rer un rapport.
"""

import os
from dotenv import load_dotenv 
from typing import Dict, List
from langchain_google_genai import ChatGoogleGenerativeAI
from src.utils.logger import log_experiment, ActionType

load_dotenv()

# Import des outils du Toolsmith
from src.tools.file_tools import read_file_safe, list_python_files
from src.tools.pylint_tool import run_pylint, parse_pylint_output

# Import du prompt syst√®me
try:
    from src.prompts.auditor_prompts import AUDITOR_SYSTEM_PROMPT
except ImportError:
    # Prompt de secours si le Prompt Engineer n'a pas encore cr√©√© le fichier
    AUDITOR_SYSTEM_PROMPT = """Tu es un expert Python charg√© d'analyser du code.

MISSION :
Analyse le code fourni et les rapports pylint, puis g√©n√®re un rapport JSON.

FORMAT DE SORTIE (STRICT) :
{
    "files_analyzed": ["file1.py", "file2.py"],
    "total_issues": 5,
    "issues": [
        {
            "file": "example.py",
            "line": 10,
            "severity": "high",
            "type": "missing_docstring",
            "message": "La fonction manque de docstring"
        }
    ],
    "recommendations": ["Ajouter des docstrings", "Corriger la syntaxe"]
}

R√©ponds UNIQUEMENT avec du JSON valide, sans texte avant ou apr√®s."""


class AuditorAgent:
    """
    Agent responsable de l'audit du code.
    Analyse la qualit√©, d√©tecte les bugs et g√©n√®re un rapport.
    """
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialise l'agent auditeur.
        
        Args:
            model_name: Nom du mod√®le LLM √† utiliser (d√©faut: gemini-1.5-flash)
        """
        self.model_name = model_name
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=0.1,  # Basse temp√©rature pour plus de pr√©cision
            convert_system_message_to_human=True
        )
        print(f"‚úÖ AuditorAgent initialis√© avec le mod√®le : {model_name}")
    
    def analyze(self, target_dir: str) -> Dict:
        """
        Analyse tous les fichiers Python d'un dossier.
        
        Args:
            target_dir: Chemin du dossier √† analyser
            
        Returns:
            Dict: Rapport d'audit contenant les probl√®mes d√©tect√©s
        """
        print(f"\nüîç [AUDITOR] D√©marrage de l'analyse de : {target_dir}")
        
        try:
            # √âtape 1 : Lister les fichiers Python
            print("üìÇ Recherche des fichiers Python...")
            python_files = list_python_files(target_dir)
            
            if not python_files:
                print("‚ö†Ô∏è  Aucun fichier Python trouv√© dans le dossier.")
                return {
                    "files_analyzed": [],
                    "total_issues": 0,
                    "issues": [],
                    "recommendations": ["Aucun fichier Python √† analyser"]
                }
            
            print(f"‚úÖ {len(python_files)} fichier(s) Python trouv√©(s)")
            
            # √âtape 2 : Analyser chaque fichier
            all_issues = []
            files_analyzed = []
            
            for filename in python_files:
                print(f"\nüìÑ Analyse de : {filename}")
                
                # Construire le chemin complet
                full_path = os.path.join(target_dir, filename)
                
                # Lire le contenu du fichier
                file_content = read_file_safe(full_path, target_dir)  
                
                # Lancer pylint et parser le r√©sultat
                pylint_output = run_pylint(full_path)
                pylint_result = parse_pylint_output(pylint_output)
                
                # Construire le prompt pour le LLM
                user_prompt = self._build_analysis_prompt(
                    filename=filename,
                    file_content=file_content,
                    pylint_result=pylint_result
                )
                
                # Appeler le LLM
                print(f"ü§ñ Consultation du LLM pour l'analyse...")
                llm_response = self._call_llm(user_prompt)
                
                # Logger l'interaction
                log_experiment(
                    agent_name="Auditor_Agent",
                    model_used=self.model_name,
                    action=ActionType.ANALYSIS,
                    details={
                        "file_analyzed": filename,
                        "pylint_score": pylint_result.get("score", 0),
                        "input_prompt": user_prompt,
                        "output_response": llm_response,
                    },
                    status="SUCCESS"
                )
                
                # Parser la r√©ponse du LLM
                file_issues = self._parse_llm_response(llm_response, filename)
                all_issues.extend(file_issues)
                files_analyzed.append(filename)
                
                print(f"‚úÖ Analyse termin√©e : {len(file_issues)} probl√®me(s) d√©tect√©(s)")
            
            # √âtape 3 : G√©n√©rer le rapport final
            report = {
                "files_analyzed": files_analyzed,
                "total_issues": len(all_issues),
                "issues": all_issues,
                "recommendations": self._generate_recommendations(all_issues)
            }
            
            print(f"\n‚úÖ [AUDITOR] Analyse termin√©e : {report['total_issues']} probl√®me(s) au total")
            return report
            
        except Exception as e:
            print(f"‚ùå [AUDITOR] Erreur lors de l'analyse : {str(e)}")
            log_experiment(
                agent_name="Auditor_Agent",
                model_used=self.model_name,
                action=ActionType.DEBUG,
                details={
                    "input_prompt": f"Analyse du dossier {target_dir}",
                    "output_response": f"Erreur : {str(e)}",
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            raise
    
    def _build_analysis_prompt(self, filename: str, file_content: str, 
                               pylint_result: Dict) -> str:
        """
        Construit le prompt pour l'analyse d'un fichier.
        
        Args:
            filename: Nom du fichier
            file_content: Contenu du fichier
            pylint_result: R√©sultat de l'analyse pylint
            
        Returns:
            str: Prompt format√© pour le LLM
        """
        # Limiter les issues pylint pour ne pas surcharger le prompt
        pylint_issues = pylint_result.get('issues', [])
        issues_summary = pylint_issues[:10] if len(pylint_issues) > 10 else pylint_issues
        
        return f"""Analyse ce code Python et le rapport pylint.

FICHIER : {filename}
SCORE PYLINT : {pylint_result.get('score', 0)}/10

CODE :
```python
{file_content}
```

ERREURS PYLINT (√©chantillon) :
{issues_summary}

G√©n√®re un rapport JSON avec les probl√®mes d√©tect√©s les plus importants."""
    
    def _call_llm(self, prompt: str) -> str:
        """
        Appelle le LLM avec le prompt syst√®me et le prompt utilisateur.
        
        Args:
            prompt: Prompt utilisateur
            
        Returns:
            str: R√©ponse du LLM
        """
        messages = [
            {"role": "system", "content": AUDITOR_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = self.llm.invoke(messages)
        return response.content
    
    def _parse_llm_response(self, response: str, filename: str) -> List[Dict]:
        """
        Parse la r√©ponse JSON du LLM.
        
        Args:
            response: R√©ponse brute du LLM
            filename: Nom du fichier analys√©
            
        Returns:
            List[Dict]: Liste des probl√®mes d√©tect√©s
        """
        import json
        
        try:
            # Nettoyer la r√©ponse (enlever les balises markdown si pr√©sentes)
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            data = json.loads(cleaned)
            issues = data.get("issues", [])
            
            # S'assurer que chaque issue a le nom du fichier
            for issue in issues:
                if "file" not in issue:
                    issue["file"] = filename
            
            return issues
            
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  Impossible de parser la r√©ponse JSON du LLM : {str(e)}")
            # En cas d'√©chec, cr√©er au moins un issue basique
            return [{
                "file": filename,
                "line": 0,
                "severity": "medium",
                "type": "parse_error",
                "message": "Erreur lors de l'analyse LLM"
            }]
    
    def _generate_recommendations(self, issues: List[Dict]) -> List[str]:
        """
        G√©n√®re des recommandations bas√©es sur les probl√®mes d√©tect√©s.
        
        Args:
            issues: Liste des probl√®mes
            
        Returns:
            List[str]: Recommandations
        """
        if not issues:
            return ["Le code semble conforme aux standards Python"]
        
        recommendations = []
        issue_types = set(issue.get("type", "") for issue in issues)
        
        if "missing_docstring" in issue_types:
            recommendations.append("Ajouter des docstrings aux fonctions et classes")
        if "syntax_error" in issue_types:
            recommendations.append("Corriger les erreurs de syntaxe")
        if "naming_convention" in issue_types:
            recommendations.append("Respecter les conventions de nommage PEP8")
        if "import_error" in issue_types:
            recommendations.append("V√©rifier les imports")
        if "unused_variable" in issue_types:
            recommendations.append("Supprimer les variables non utilis√©es")
            
        return recommendations if recommendations else ["Am√©liorer la qualit√© g√©n√©rale du code"]