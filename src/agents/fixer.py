"""
Agent Correcteur (The Fixer)
R√¥le : Corriger le code Python selon le rapport d'audit ET g√©n√©rer du nouveau contenu.
"""

import os
from dotenv import load_dotenv 
from typing import Dict, List
from langchain_google_genai import ChatGoogleGenerativeAI
from src.utils.logger import log_experiment, ActionType

load_dotenv()

from src.tools.file_tools import read_file_safe, write_file_safe

try:
    from src.prompts.fixer_prompts import FIXER_SYSTEM_PROMPT
except ImportError:
    FIXER_SYSTEM_PROMPT = """Tu es un expert Python charg√© de corriger du code.

MISSION :
Corrige le code Python selon les probl√®mes indiqu√©s dans le rapport d'audit.

R√àGLES IMPORTANTES :
1. Ne change PAS la logique m√©tier du code
2. Corrige uniquement les probl√®mes list√©s
3. Ajoute les docstrings manquantes
4. Respecte PEP8
5. Conserve tous les noms de fonctions/classes existants
6. Ne supprime aucune fonctionnalit√©

FORMAT DE SORTIE :
Retourne UNIQUEMENT le code Python corrig√©, sans explication ni commentaire.
Ne mets pas de balises ```python, juste le code brut."""


class FixerAgent:
    """
    Agent responsable de la correction du code et de la g√©n√©ration de contenu.
    Applique les corrections bas√©es sur le rapport d'audit ET peut cr√©er des tests/documentation.
    """
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialise l'agent correcteur.
        
        Args:
            model_name: Nom du mod√®le LLM √† utiliser
        """
        self.model_name = model_name
        
        api_key = os.getenv("GOOGLE_API_KEY")
        
        if not api_key:
            raise ValueError(
                "‚ùå Cl√© API Google non trouv√©e. "
                "Assurez-vous d'avoir GOOGLE_API_KEY dans votre fichier .env"
            )
        
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.2,
            convert_system_message_to_human=True 
        )
        print(f"‚úÖ FixerAgent initialis√© avec le mod√®le : {model_name}")
    
    def fix(self, audit_report: Dict, target_dir: str) -> Dict:
        """
        Corrige les fichiers selon le rapport d'audit (ActionType.FIX).
        
        Args:
            audit_report: Rapport g√©n√©r√© par l'Auditor
            target_dir: Dossier contenant les fichiers √† corriger
            
        Returns:
            Dict: R√©sum√© des corrections effectu√©es
        """
        print(f"\nüîß [FIXER] D√©marrage des corrections...")
        
        try:
            issues = audit_report.get("issues", [])
            
            if not issues:
                print("‚úÖ Aucun probl√®me √† corriger.")
                return {
                    "files_fixed": [],
                    "total_fixes": 0,
                    "status": "no_issues"
                }
            
            print(f"üìù {len(issues)} probl√®me(s) √† corriger")
            
            # Grouper les probl√®mes par fichier
            issues_by_file = self._group_issues_by_file(issues)
            
            files_fixed = []
            total_fixes = 0
            
            # Corriger chaque fichier
            for filename, file_issues in issues_by_file.items():
                print(f"\nüìÑ Correction de : {filename}")
                
                filepath = os.path.join(target_dir, filename)
                
                try:
                    original_content = read_file_safe(filepath, target_dir)
                except Exception as e:
                    print(f"‚ö†Ô∏è  Impossible de lire {filename} : {str(e)}")
                    continue
                
                # Construire le prompt de correction
                user_prompt = self._build_fix_prompt(
                    filename=filename,
                    original_content=original_content,
                    issues=file_issues
                )
                
                # G√©n√©rer le code corrig√©
                print(f"  ü§ñ G√©n√©ration du code corrig√©...")
                fixed_content = self._call_llm(user_prompt)
                fixed_content = self._clean_code_response(fixed_content)
                
                # Sauvegarder
                try:
                    write_file_safe(filepath, fixed_content, target_dir)
                    print(f"  ‚úÖ Fichier corrig√© et sauvegard√©")
                    files_fixed.append(filename)
                    total_fixes += len(file_issues)
                except Exception as e:
                    print(f"  ‚ùå Erreur lors de l'√©criture de {filename} : {str(e)}")
                    continue
                
                # ‚úÖ Logger avec ActionType.FIX (on MODIFIE du code existant)
                log_experiment(
                    agent_name="Fixer_Agent",
                    model_used=self.model_name,
                    action=ActionType.FIX,  # ‚úÖ On MODIFIE du code existant
                    details={
                        "file_analyzed": filename,
                        "input_prompt": user_prompt,
                        "output_response": fixed_content[:500] + "..." if len(fixed_content) > 500 else fixed_content,
                        "issues_found": len(file_issues),  # Erreurs qu'on a corrig√©es
                        "issues_types": [issue.get("type") for issue in file_issues]
                    },
                    status="SUCCESS"
                )
            
            result = {
                "files_fixed": files_fixed,
                "total_fixes": total_fixes,
                "status": "completed"
            }
            
            print(f"\n‚úÖ [FIXER] Corrections termin√©es : {total_fixes} probl√®me(s) corrig√©(s) dans {len(files_fixed)} fichier(s)")
            return result
            
        except Exception as e:
            print(f"‚ùå [FIXER] Erreur lors des corrections : {str(e)}")
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.DEBUG,
                details={
                    "file_analyzed": "system_error",
                    "input_prompt": f"Correction des fichiers dans {target_dir}",
                    "output_response": f"Erreur : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            raise
    
    def generate_tests(self, filename: str, target_dir: str) -> str:
        """
        G√©n√®re des tests unitaires pour un fichier (ActionType.GENERATION).
        CR√âE un NOUVEAU fichier qui n'existait pas.
        
        Args:
            filename: Nom du fichier source
            target_dir: Dossier cible
            
        Returns:
            str: Contenu des tests g√©n√©r√©s
        """
        print(f"\nüß™ [FIXER] G√©n√©ration de tests pour : {filename}")
        
        try:
            filepath = os.path.join(target_dir, filename)
            code_content = read_file_safe(filepath, target_dir)
            
            # Prompt pour cr√©er des tests
            prompt = f"""G√©n√®re des tests unitaires pytest pour ce code Python :

FICHIER : {filename}

CODE :
```python
{code_content}
```

INSTRUCTIONS :
1. Cr√©e au moins 5 tests unitaires complets
2. Utilise pytest et les fixtures si n√©cessaire
3. Teste les cas normaux ET les cas d'erreur
4. Nomme les tests de fa√ßon explicite (test_nom_fonction_cas)
5. Ajoute des docstrings aux tests
6. Import toutes les d√©pendances n√©cessaires

Retourne UNIQUEMENT le code des tests, sans explication."""
            
            # Appeler le LLM
            test_content = self._call_llm(prompt)
            test_content = self._clean_code_response(test_content)
            
            # Sauvegarder dans un NOUVEAU fichier
            test_filename = f"test_{filename}"
            test_filepath = os.path.join(target_dir, test_filename)
            write_file_safe(test_filepath, test_content, target_dir)
            
            # ‚úÖ Logger avec ActionType.GENERATION (on CR√âE du nouveau contenu)
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.GENERATION,  # ‚úÖ On CR√âE un nouveau fichier
                details={
                    "file_analyzed": test_filename,  # Le NOUVEAU fichier cr√©√©
                    "input_prompt": prompt,
                    "output_response": test_content[:500] + "..." if len(test_content) > 500 else test_content,
                    "issues_found": 0,  # Pas d'erreurs, c'est du nouveau contenu
                    "source_file": filename,
                    "content_type": "unit_tests"
                },
                status="SUCCESS"
            )
            
            print(f"‚úÖ Tests g√©n√©r√©s : {test_filename}")
            return test_content
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de tests : {str(e)}")
            
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.GENERATION,
                details={
                    "file_analyzed": f"test_{filename}",
                    "input_prompt": f"G√©n√©ration de tests pour {filename}",
                    "output_response": f"Erreur : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            raise
    
    def generate_documentation(self, filename: str, target_dir: str) -> str:
        """
        G√©n√®re de la documentation pour un fichier (ActionType.GENERATION).
        CR√âE un NOUVEAU fichier de documentation.
        
        Args:
            filename: Nom du fichier source
            target_dir: Dossier cible
            
        Returns:
            str: Contenu de la documentation
        """
        print(f"\nüìù [FIXER] G√©n√©ration de documentation pour : {filename}")
        
        try:
            filepath = os.path.join(target_dir, filename)
            code_content = read_file_safe(filepath, target_dir)
            
            prompt = f"""G√©n√®re une documentation compl√®te pour ce code Python :

FICHIER : {filename}

CODE :
```python
{code_content}
```

INSTRUCTIONS :
Cr√©e un README.md avec :
1. ## Description
   - R√©sum√© du module/fichier
   - Objectif principal

2. ## Fonctions principales
   - Liste des fonctions avec description
   - Param√®tres et retours

3. ## Utilisation
   - Exemples de code concrets
   - Cas d'usage typiques

4. ## D√©pendances
   - Liste des imports n√©cessaires

Format Markdown strict. Sois concis mais complet."""
            
            doc_content = self._call_llm(prompt)
            
            # Sauvegarder dans un NOUVEAU fichier
            doc_filename = f"README_{filename.replace('.py', '')}.md"
            doc_filepath = os.path.join(target_dir, doc_filename)
            write_file_safe(doc_filepath, doc_content, target_dir)
            
            # ‚úÖ Logger avec ActionType.GENERATION
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.GENERATION,
                details={
                    "file_analyzed": doc_filename,
                    "input_prompt": prompt,
                    "output_response": doc_content[:500] + "..." if len(doc_content) > 500 else doc_content,
                    "issues_found": 0,
                    "source_file": filename,
                    "content_type": "documentation"
                },
                status="SUCCESS"
            )
            
            print(f"‚úÖ Documentation g√©n√©r√©e : {doc_filename}")
            return doc_content
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de documentation : {str(e)}")
            
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.GENERATION,
                details={
                    "file_analyzed": f"README_{filename.replace('.py', '')}.md",
                    "input_prompt": f"G√©n√©ration de documentation pour {filename}",
                    "output_response": f"Erreur : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            raise
    
    def retry_fix(self, filepath: str, target_dir: str, error_message: str) -> str:
        """
        R√©essaye de corriger un fichier suite √† une erreur (ActionType.FIX).
        
        Args:
            filepath: Chemin du fichier
            target_dir: Dossier cible
            error_message: Message d'erreur du test pr√©c√©dent
            
        Returns:
            str: Code corrig√©
        """
        print(f"\nüîÑ [FIXER] Nouvelle tentative de correction pour : {os.path.basename(filepath)}")
        
        try:
            original_content = read_file_safe(filepath, target_dir)
            
            retry_prompt = f"""Le code pr√©c√©dent a √©chou√© aux tests.

FICHIER : {os.path.basename(filepath)}
ERREUR RENCONTR√âE :
{error_message}

CODE ACTUEL :
```python
{original_content}
```

Analyse l'erreur et corrige le code. Retourne uniquement le code Python corrig√©."""
            
            fixed_content = self._call_llm(retry_prompt)
            fixed_content = self._clean_code_response(fixed_content)
            
            write_file_safe(filepath, fixed_content, target_dir)
            
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.FIX,
                details={
                    "file_analyzed": os.path.basename(filepath),
                    "input_prompt": retry_prompt,
                    "output_response": fixed_content[:500] + "..." if len(fixed_content) > 500 else fixed_content,
                    "issues_found": 1,  # L'erreur qu'on essaie de corriger
                    "retry": True,
                    "error_message": error_message[:200]
                },
                status="SUCCESS"
            )
            
            print(f"‚úÖ Nouvelle version g√©n√©r√©e")
            return fixed_content
            
        except Exception as e:
            print(f"‚ùå √âchec de la nouvelle tentative : {str(e)}")
            
            log_experiment(
                agent_name="Fixer_Agent",
                model_used=self.model_name,
                action=ActionType.FIX,
                details={
                    "file_analyzed": os.path.basename(filepath),
                    "input_prompt": f"Retry fix pour {os.path.basename(filepath)}",
                    "output_response": f"Erreur : {str(e)}",
                    "issues_found": 1,
                    "retry": True
                },
                status="FAILURE"
            )
            raise
    
    def _group_issues_by_file(self, issues: List[Dict]) -> Dict[str, List[Dict]]:
        """Regroupe les probl√®mes par fichier."""
        grouped = {}
        for issue in issues:
            filename = issue.get("file", "unknown.py")
            if filename not in grouped:
                grouped[filename] = []
            grouped[filename].append(issue)
        return grouped
    
    def _build_fix_prompt(self, filename: str, original_content: str, 
                          issues: List[Dict]) -> str:
        """Construit le prompt pour corriger un fichier."""
        issues_text = "\n".join([
            f"- Ligne {issue.get('line', '?')} : {issue.get('message', 'Probl√®me non sp√©cifi√©')} "
            f"(Type: {issue.get('type', 'unknown')}, S√©v√©rit√©: {issue.get('severity', 'medium')})"
            for issue in issues
        ])
        
        return f"""Corrige ce code Python selon les probl√®mes d√©tect√©s.

FICHIER : {filename}
NOMBRE DE PROBL√àMES : {len(issues)}

PROBL√àMES √Ä CORRIGER :
{issues_text}

CODE ACTUEL :
```python
{original_content}
```

INSTRUCTIONS :
1. Corrige tous les probl√®mes list√©s
2. Ajoute les docstrings manquantes (format Google/NumPy style)
3. Respecte PEP8 (espaces, nommage, etc.)
4. Ne change PAS la logique du code
5. Conserve tous les noms de fonctions/variables

Retourne uniquement le code Python corrig√©, sans explication."""
    
    def _call_llm(self, prompt: str) -> str:
        """Appelle le LLM."""
        messages = [
            {"role": "system", "content": FIXER_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = self.llm.invoke(messages)
        
        if isinstance(response.content, list):
            content = response.content[0] if response.content else ""
            if hasattr(content, 'text'):
                return content.text
            return str(content)
        
        return response.content
    
    def _clean_code_response(self, response: str) -> str:
        """Nettoie la r√©ponse du LLM (enl√®ve les balises markdown)."""
        cleaned = response.strip()
        
        if cleaned.startswith("```python"):
            cleaned = cleaned[9:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        
        return cleaned.strip()
