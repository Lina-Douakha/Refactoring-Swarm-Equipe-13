"""
Agent Testeur (The Judge)
R√¥le : Ex√©cuter les tests unitaires et valider que le code fonctionne.
"""

import os
from typing import Dict, List, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI
from src.utils.logger import log_experiment, ActionType
from dotenv import load_dotenv  
# Import des outils du Toolsmith
try:
    from src.tools.pytest_tool import run_pytest
    from src.tools.file_tools import read_file_safe
except ImportError:
    print("   ATTENTION : Les outils du Toolsmith ne sont pas encore disponibles.")
    print("   Les fonctions suivantes doivent √™tre cr√©√©es :")
    print("   - src/tools/pytest_tool.py : run_pytest()")
    print("   - src/tools/file_tools.py : read_file_safe()")

# Import du prompt syst√®me
try:
    from src.prompts.judge_prompts import JUDGE_SYSTEM_PROMPT
except ImportError:
    # Prompt de secours
    JUDGE_SYSTEM_PROMPT = """Tu es un expert en debugging Python et analyse de tests.

MISSION :
Analyse les r√©sultats de tests pytest et identifie la cause des √©checs.

INSTRUCTIONS :
1. Lis attentivement les messages d'erreur
2. Identifie le type d'erreur (AssertionError, TypeError, etc.)
3. Localise la ligne probl√©matique
4. Propose une solution claire et pr√©cise

FORMAT DE SORTIE (JSON) :
{
    "error_type": "AssertionError",
    "affected_function": "test_add",
    "root_cause": "La fonction add() retourne un string au lieu d'un int",
    "recommendation": "Convertir le r√©sultat en int avant de le retourner",
    "severity": "high"
}

R√©ponds UNIQUEMENT avec du JSON valide."""


class JudgeAgent:
    """
    Agent responsable de l'ex√©cution et validation des tests.
    Valide que le code corrig√© fonctionne correctement.
    """
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialise l'agent testeur.
        
        Args:
            model_name: Nom du mod√®le LLM √† utiliser
        """
        self.model_name = model_name
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            temperature=0.1,  # Basse temp√©rature pour analyse pr√©cise
        )
        print(f"‚öñÔ∏è  JudgeAgent initialis√© avec le mod√®le : {model_name}")
    
    def test(self, target_dir: str) -> Dict:
        """
        Ex√©cute les tests unitaires sur le code.
        
        Args:
            target_dir: Dossier contenant le code √† tester
            
        Returns:
            Dict: R√©sultat des tests avec statut et d√©tails
        """
        print(f"\n‚öñÔ∏è  [JUDGE] D√©marrage des tests sur : {target_dir}")
        
        try:
            # √âtape 1 : Ex√©cuter pytest
            print("üß™ Ex√©cution de pytest...")
            test_result = run_pytest(target_dir)
            
            passed = test_result.get("passed", 0)
            failed = test_result.get("failed", 0)
            total = passed + failed
            
            print(f" R√©sultats : {passed}/{total} tests r√©ussis")
            
            # √âtape 2 : Analyser les r√©sultats
            if test_result.get("success", False):
                #    SUCC√àS - Tous les tests passent
                print("üéâ [JUDGE] Tous les tests passent !")
                
                result = {
                    "success": True,
                    "passed": passed,
                    "failed": 0,
                    "errors": [],
                    "recommendations": []
                }
                
                # Logger le succ√®s
                log_experiment(
                    agent_name="Judge_Agent",
                    model_used=self.model_name,
                    action=ActionType.DEBUG,
                    details={
                        "test_directory": target_dir,
                        "input_prompt": f"Validation des tests dans {target_dir}",
                        "output_response": f"Tous les tests r√©ussis : {passed}/{total}",
                        "passed": passed,
                        "failed": 0
                    },
                    status="SUCCESS"
                )
                
                return result
            
            else:
                #    √âCHEC - Des tests ont √©chou√©
                print(f"   [JUDGE] {failed} test(s) ont √©chou√©")
                
                errors = test_result.get("errors", [])
                
                # √âtape 3 : Analyser les erreurs avec le LLM
                print("  Analyse des erreurs avec le LLM...")
                
                #    CORRECTION ICI : R√©cup√©rer les DEUX valeurs
                analysis, llm_raw_response = self._analyze_test_failures(errors, target_dir)
                
                result = {
                    "success": False,
                    "passed": passed,
                    "failed": failed,
                    "errors": errors,
                    "recommendations": analysis.get("recommendations", []),
                    "root_causes": analysis.get("root_causes", [])
                }
                
                # Logger l'√©chec
                log_experiment(
                    agent_name="Judge_Agent",
                    model_used=self.model_name,
                    action=ActionType.DEBUG,
                    details={
                        "test_directory": target_dir,
                        "input_prompt": self._build_analysis_prompt(errors),
                        "output_response": llm_raw_response,  #    R√âPONSE BRUTE DU LLM
                        "passed": passed,
                        "failed": failed,
                        "errors_sample": errors[:3] if len(errors) > 3 else errors
                    },
                    status="FAILURE"
                )
                
                return result
                
        except Exception as e:
            print(f"   [JUDGE] Erreur lors de l'ex√©cution des tests : {str(e)}")
            
            log_experiment(
                agent_name="Judge_Agent",
                model_used=self.model_name,
                action=ActionType.DEBUG,
                details={
                    "test_directory": target_dir,
                    "input_prompt": f"Ex√©cution de pytest sur {target_dir}",
                    "output_response": f"Erreur : {str(e)}",
                    "error_type": type(e).__name__
                },
                status="FAILURE"
            )
            
            # Retourner un √©chec avec l'erreur
            return {
                "success": False,
                "passed": 0,
                "failed": 0,
                "errors": [str(e)],
                "recommendations": ["V√©rifier que pytest est correctement install√© et que les tests sont valides"]
            }
    
    def _analyze_test_failures(self, errors: List[str], target_dir: str) -> Tuple[Dict, str]:
        """
        Analyse les √©checs de tests avec le LLM.
        
        Args:
            errors: Liste des messages d'erreur
            target_dir: Dossier contenant le code
            
        Returns:
            Tuple[Dict, str]: (analyse_structur√©e, r√©ponse_brute_du_LLM)
        """
        try:
            # Construire le prompt d'analyse
            analysis_prompt = self._build_analysis_prompt(errors)
            
            # Appeler le LLM
            llm_response = self._call_llm(analysis_prompt)
            
            # Parser la r√©ponse
            analysis = self._parse_analysis_response(llm_response)
            
            #    Retourner les DEUX : le dictionnaire ET la r√©ponse brute
            return analysis, llm_response
            
        except Exception as e:
            print(f"   Erreur lors de l'analyse LLM : {str(e)}")
            return {
                "recommendations": ["Corriger les erreurs de test"],
                "root_causes": ["Erreur d'analyse"]
            }, f"Erreur : {str(e)}"
    
    def _build_analysis_prompt(self, errors: List[str]) -> str:
        """
        Construit le prompt pour analyser les erreurs de tests.
        
        Args:
            errors: Liste des messages d'erreur
            
        Returns:
            str: Prompt format√© pour le LLM
        """
        errors_text = "\n\n".join([
            f"ERREUR {i+1}:\n{error}" 
            for i, error in enumerate(errors[:5])  # Limiter √† 5 erreurs max
        ])
        
        return f"""Analyse ces erreurs de tests pytest et identifie les causes.

NOMBRE D'ERREURS : {len(errors)}

MESSAGES D'ERREUR :
{errors_text}

INSTRUCTIONS :
1. Identifie le type d'erreur (AssertionError, TypeError, NameError, etc.)
2. Localise la fonction/ligne probl√©matique
3. D√©termine la cause racine
4. Propose une solution concr√®te

G√©n√®re un rapport JSON avec :
- recommendations : liste de solutions
- root_causes : liste des causes identifi√©es
- severity : "low", "medium" ou "high"
"""
    
    def _call_llm(self, prompt: str) -> str:
        """
        Appelle le LLM pour analyser les erreurs.
        
        Args:
            prompt: Prompt utilisateur
            
        Returns:
            str: Analyse du LLM
        """
        messages = [
            {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = self.llm.invoke(messages)
        return response.content
    
    def _parse_analysis_response(self, response: str) -> Dict:
        """
        Parse la r√©ponse JSON du LLM.
        
        Args:
            response: R√©ponse brute du LLM
            
        Returns:
            Dict: Analyse structur√©e
        """
        import json
        
        try:
            # Nettoyer la r√©ponse
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            data = json.loads(cleaned)
            
            # Extraire les informations pertinentes
            return {
                "recommendations": data.get("recommendations", [data.get("recommendation", "Corriger les erreurs")]),
                "root_causes": [data.get("root_cause", "Cause inconnue")],
                "severity": data.get("severity", "medium")
            }
            
        except json.JSONDecodeError:
            print("   Impossible de parser la r√©ponse JSON du LLM")
            # Extraire au moins du texte utile
            return {
                "recommendations": [response[:200]] if response else ["Corriger les erreurs de test"],
                "root_causes": ["Analyse non structur√©e"],
                "severity": "unknown"
            }
    
    def validate_code_quality(self, target_dir: str, min_score: float = 7.0) -> bool:
        """
        BONUS : Valide la qualit√© du code avec pylint (optionnel).
        
        Args:
            target_dir: Dossier √† valider
            min_score: Score minimum acceptable (sur 10)
            
        Returns:
            bool: True si la qualit√© est acceptable
        """
        print(f"\n [JUDGE] Validation de la qualit√© du code (score minimum : {min_score}/10)")
        
        try:
            from src.tools.pylint_tool import run_pylint
            from src.tools.file_tools import list_python_files
            
            python_files = list_python_files(target_dir)
            total_score = 0
            
            for filepath in python_files:
                result = run_pylint(filepath)
                score = result.get("score", 0)
                total_score += score
                print(f"  - {os.path.basename(filepath)}: {score}/10")
            
            average_score = total_score / len(python_files) if python_files else 0
            print(f"\n Score moyen : {average_score:.2f}/10")
            
            return average_score >= min_score
            
        except Exception as e:
            print(f"  Validation de qualit√© impossible : {str(e)}")
            return True  # Ne pas bloquer si la validation √©choue