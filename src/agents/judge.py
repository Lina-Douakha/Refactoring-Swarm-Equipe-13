# """
# Agent Testeur (The Judge)
# R√¥le : Ex√©cuter les tests unitaires et valider que le code fonctionne.
# """

# import os
# from typing import Dict, List, Tuple
# from langchain_google_genai import ChatGoogleGenerativeAI
# from src.utils.logger import log_experiment, ActionType
# from dotenv import load_dotenv

# load_dotenv()


# try:
#     from src.tools.pytest_tool import run_pytest
#     from src.tools.file_tools import read_file_safe
# except ImportError:
#     print("  ATTENTION : Les outils du Toolsmith ne sont pas encore disponibles.")
#     print("   Les fonctions suivantes doivent √™tre cr√©√©es :")
#     print("   - src/tools/pytest_tool.py : run_pytest()")
#     print("   - src/tools/file_tools.py : read_file_safe()")


# try:
#     from src.prompts.judge_prompts import JUDGE_SYSTEM_PROMPT
# except ImportError:
#     JUDGE_SYSTEM_PROMPT = """Tu es un expert en debugging Python et analyse de tests.

# MISSION :
# Analyse les r√©sultats de tests pytest et identifie la cause des √©checs.

# INSTRUCTIONS :
# 1. Lis attentivement les messages d'erreur
# 2. Identifie le type d'erreur (AssertionError, TypeError, etc.)
# 3. Localise la ligne probl√©matique
# 4. Propose une solution claire et pr√©cise

# FORMAT DE SORTIE (JSON) :
# {
#     "recommendations": ["Corriger X", "V√©rifier Y"],
#     "root_causes": ["Cause 1", "Cause 2"],
#     "severity": "high"
# }

# R√©ponds UNIQUEMENT avec du JSON valide."""


# class JudgeAgent:
#     """
#     Agent responsable de l'ex√©cution et validation des tests.
#     Valide que le code corrig√© fonctionne correctement.
#     """
    
#     def __init__(self, model_name: str = "gemini-2.5-flash-lite"):
#         """
#         Initialise l'agent testeur.
        
#         Args:
#             model_name: Nom du mod√®le LLM √† utiliser
#         """
#         self.model_name = model_name
        
#         api_key = os.getenv("GOOGLE_API_KEY")
        
#         if not api_key:
#             raise ValueError(
#                 " Cl√© API Google non trouv√©e. "
#                 "Assurez-vous d'avoir GOOGLE_API_KEY dans votre fichier .env"
#             )
        
#         self.llm = ChatGoogleGenerativeAI(
#             model=model_name,
#             google_api_key=api_key,
#             temperature=0.1,
#         )
#         print(f"  JudgeAgent initialis√© avec le mod√®le : {model_name}")
    
#     def test(self, target_dir: str) -> Dict:
#         """
#         Ex√©cute les tests unitaires sur le code.
        
#         Args:
#             target_dir: Dossier contenant le code √† tester
            
#         Returns:
#             Dict: R√©sultat des tests avec statut et d√©tails
#         """
#         print(f"\n  [JUDGE] D√©marrage des tests sur : {target_dir}")
        
#         try:
            
#             print(" Ex√©cution de pytest...")
#             test_result = run_pytest(target_dir)
            
#             passed = test_result.get("passed", 0)
#             failed = test_result.get("failed", 0)
#             total = passed + failed
            
#             print(f" R√©sultats : {passed}/{total} tests r√©ussis")
            
            
#             if test_result.get("success", False):
#                 print(" [JUDGE] Tous les tests passent !")
                
#                 result = {
#                     "success": True,
#                     "passed": passed,
#                     "failed": 0,
#                     "errors": [],
#                     "recommendations": []
#                 }
                
                
                
#                 log_experiment(
#                     agent_name="Judge_Agent",
#                     model_used=self.model_name,
#                     action=ActionType.ANALYSIS,  
#                     details={
#                         "test_directory": target_dir,
#                         "input_prompt": f"Ex√©cution et validation de pytest dans {target_dir}",
#                         "output_response": f"R√©sultat: {passed} tests r√©ussis sur {total}. Tous les tests passent.",
#                         "passed": passed,
#                         "failed": 0,
#                         "total": total
#                     },
#                     status="SUCCESS"
#                 )
                
#                 return result
            
#             else:
#                 print(f" [JUDGE] {failed} test(s) ont √©chou√©")
                
#                 errors = test_result.get("errors", [])
                
#                 print(" Analyse des erreurs avec le LLM...")
#                 analysis, llm_raw_response = self._analyze_test_failures(errors, target_dir)
                
#                 result = {
#                     "success": False,
#                     "passed": passed,
#                     "failed": failed,
#                     "errors": errors,
#                     "recommendations": analysis.get("recommendations", []),
#                     "root_causes": analysis.get("root_causes", [])
#                 }
                
#                 log_experiment(
#                     agent_name="Judge_Agent",
#                     model_used=self.model_name,
#                     action=ActionType.DEBUG,
#                     details={
#                         "test_directory": target_dir,
#                         "input_prompt": self._build_analysis_prompt(errors),
#                         "output_response": llm_raw_response,
#                         "passed": passed,
#                         "failed": failed,
#                         "errors_sample": errors[:3] if len(errors) > 3 else errors
#                     },
#                     status="SUCCESS"
#                 )
                
#                 return result
                
#         except Exception as e:
#             print(f" [JUDGE] Erreur lors de l'ex√©cution des tests : {str(e)}")
            
#             log_experiment(
#                 agent_name="Judge_Agent",
#                 model_used=self.model_name,
#                 action=ActionType.DEBUG,
#                 details={
#                     "test_directory": target_dir,
#                     "input_prompt": f"Ex√©cution de pytest sur {target_dir}",
#                     "output_response": f"Erreur : {str(e)}",
#                     "error_type": type(e).__name__
#                 },
#                 status="FAILURE"
#             )
            
#             return {
#                 "success": False,
#                 "passed": 0,
#                 "failed": 0,
#                 "errors": [str(e)],
#                 "recommendations": ["V√©rifier que pytest est correctement install√© et que les tests sont valides"]
#             }
    
#     def _analyze_test_failures(self, errors: List[str], target_dir: str) -> Tuple[Dict, str]:
#         """Analyse les √©checs de tests avec le LLM."""
#         try:
#             analysis_prompt = self._build_analysis_prompt(errors)
#             llm_response = self._call_llm(analysis_prompt)
#             analysis = self._parse_analysis_response(llm_response)
#             return analysis, llm_response
#         except Exception as e:
#             print(f"  Erreur lors de l'analyse LLM : {str(e)}")
#             return {
#                 "recommendations": ["Corriger les erreurs de test"],
#                 "root_causes": ["Erreur d'analyse"]
#             }, f"Erreur : {str(e)}"
    
#     def _build_analysis_prompt(self, errors: List[str]) -> str:
#         """Construit le prompt pour analyser les erreurs."""
#         errors_text = "\n\n".join([
#             f"ERREUR {i+1}:\n{error}" 
#             for i, error in enumerate(errors[:5])
#         ])
        
#         return f"""Analyse ces erreurs de tests pytest.

# NOMBRE D'ERREURS : {len(errors)}

# MESSAGES D'ERREUR :
# {errors_text}

# G√©n√®re un rapport JSON avec recommendations et root_causes."""
    
#     def _call_llm(self, prompt: str) -> str:
#         """Appelle le LLM."""
#         messages = [
#             {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
#             {"role": "user", "content": prompt}
#         ]
#         response = self.llm.invoke(messages)
        
#         if isinstance(response.content, list):
#             content = response.content[0] if response.content else ""
#             if hasattr(content, 'text'):
#                 return content.text
#             return str(content)
        
#         return response.content
    
#     def _parse_analysis_response(self, response: str) -> Dict:
#         """Parse la r√©ponse JSON du LLM."""
#         import json
        
#         if not isinstance(response, str):
#             print(f"  R√©ponse LLM inattendue (type: {type(response)})")
#             return {
#                 "recommendations": ["Corriger les erreurs de test"],
#                 "root_causes": ["Format de r√©ponse LLM inattendu"],
#                 "severity": "unknown"
#             }
        
#         try:
#             cleaned = response.strip()
#             if cleaned.startswith("```json"):
#                 cleaned = cleaned[7:]
#             if cleaned.startswith("```"):
#                 cleaned = cleaned[3:]
#             if cleaned.endswith("```"):
#                 cleaned = cleaned[:-3]
#             cleaned = cleaned.strip()
            
#             data = json.loads(cleaned)
            
#             return {
#                 "recommendations": data.get("recommendations", [data.get("recommendation", "Corriger les erreurs")]),
#                 "root_causes": [data.get("root_cause", "Cause inconnue")] if isinstance(data.get("root_cause"), str) else data.get("root_causes", ["Cause inconnue"]),
#                 "severity": data.get("severity", "medium")
#             }
#         except json.JSONDecodeError:
#             return {
#                 "recommendations": [response[:200]] if response else ["Corriger les erreurs"],
#                 "root_causes": ["Analyse non structur√©e"],
#                 "severity": "unknown"
#             }
"""
Agent Testeur (The Judge)
R√¥le : Ex√©cuter les tests unitaires et valider que le code fonctionne.
"""

import os
from typing import Dict, List, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI
from src.utils.logger import log_experiment, ActionType
from dotenv import load_dotenv

load_dotenv()

try:
    from src.tools.pytest_tool import run_pytest
    from src.tools.file_tools import read_file_safe
except ImportError:
    print("‚ö†Ô∏è  ATTENTION : Les outils du Toolsmith ne sont pas encore disponibles.")
    print("   Les fonctions suivantes doivent √™tre cr√©√©es :")
    print("   - src/tools/pytest_tool.py : run_pytest()")
    print("   - src/tools/file_tools.py : read_file_safe()")

try:
    from src.prompts.judge_prompts import JUDGE_SYSTEM_PROMPT
except ImportError:
    JUDGE_SYSTEM_PROMPT = """Tu es un expert en debugging Python et analyse de tests.

MISSION :
Analyse les r√©sultats de tests pytest et identifie la cause des √©checs.

INSTRUCTIONS :
1. Lis attentivement les messages d'erreur
2. Identifie le type d'erreur (AssertionError, TypeError, etc.)
3. Localise la ligne probl√©matique
4. Propose une solution claire et pr√©cise

FORMAT DE SORTIE (JSON) :
{
    "recommendations": ["Corriger X", "V√©rifier Y"],
    "root_causes": ["Cause 1", "Cause 2"],
    "severity": "high"
}

R√©ponds UNIQUEMENT avec du JSON valide."""


class JudgeAgent:
    """
    Agent responsable de l'ex√©cution et validation des tests.
    Valide que le code corrig√© fonctionne correctement.
    
    Correspond √† l'Agent Testeur (The Judge) du TP "The Refactoring Swarm".
    """
    
    def __init__(self, model_name: str = "gemini-2.0-flash-exp"):
        """
        Initialise l'agent testeur.
        
        Args:
            model_name: Nom du mod√®le LLM √† utiliser (recommand√©: gemini-2.0-flash-exp)
        """
        self.model_name = model_name
        
        api_key = os.getenv("GOOGLE_API_KEY")
        
        if not api_key:
            raise ValueError(
                "‚ùå Cl√© API Google non trouv√©e. "
                "Assurez-vous d'avoir GOOGLE_API_KEY dans votre fichier .env"
            )
        
        self.llm = ChatGoogleGenerativeAI(
            model=model_name,
            google_api_key=api_key,
            temperature=0.1,
        )
        print(f"‚úÖ  JudgeAgent initialis√© avec le mod√®le : {model_name}")
    
    def test(self, target_dir: str) -> Dict:
        """
        Ex√©cute les tests unitaires sur le code et logue chaque fichier individuellement.
        
        Cette m√©thode respecte le protocole de logging du TP en enregistrant :
        - file_analyzed : Nom du fichier Python test√©
        - issues_found : Nombre d'erreurs d√©tect√©es pour ce fichier
        - input_prompt : Le prompt envoy√© au LLM (OBLIGATOIRE)
        - output_response : La r√©ponse du LLM (OBLIGATOIRE)
        
        Args:
            target_dir: Dossier contenant le code √† tester (ex: "./sandbox/dataset_inconnu")
            
        Returns:
            Dict: R√©sultat des tests avec statut et d√©tails
        """
        print(f"\nüîç  [JUDGE] D√©marrage des tests sur : {target_dir}")
        
        try:
            # ‚úÖ √âTAPE 1 : Lister tous les fichiers Python dans le dossier
            python_files = [f for f in os.listdir(target_dir) if f.endswith(".py")]
            print(f"üìÅ Fichiers Python trouv√©s : {len(python_files)}")
            
            if len(python_files) == 0:
                print("‚ö†Ô∏è  Aucun fichier Python trouv√© dans le dossier cible.")
                return {
                    "success": False,
                    "passed": 0,
                    "failed": 0,
                    "errors": ["Aucun fichier Python trouv√©"],
                    "recommendations": ["V√©rifier que le dossier contient des fichiers .py"]
                }
            
            # ‚úÖ √âTAPE 2 : Ex√©cution de pytest sur tout le dossier
            print("‚öôÔ∏è Ex√©cution de pytest...")
            test_result = run_pytest(target_dir)
            
            passed = test_result.get("passed", 0)
            failed = test_result.get("failed", 0)
            total = passed + failed
            errors = test_result.get("errors", [])
            
            print(f"üìä R√©sultats globaux : {passed}/{total} tests r√©ussis")
            
            # ‚úÖ √âTAPE 3 : Logger chaque fichier Python individuellement
            for file_name in python_files:
                file_path = os.path.join(target_dir, file_name)
                
                try:
                    # Lire le contenu du fichier
                    code_content = read_file_safe(file_path)
                    
                    # Filtrer les erreurs li√©es √† ce fichier sp√©cifique
                    file_errors = self._filter_errors_for_file(errors, file_name)
                    file_issues_count = len(file_errors)
                    
                    # Construire le prompt d'analyse pour ce fichier
                    input_prompt = f"Analyse du fichier {file_name} dans {target_dir}. Tests ex√©cut√©s avec pytest."
                    
                    # Construire la r√©ponse
                    if file_issues_count == 0:
                        output_response = f"‚úÖ Fichier {file_name} : Aucune erreur d√©tect√©e. Tous les tests passent."
                        file_status = "SUCCESS"
                    else:
                        output_response = f"‚ùå Fichier {file_name} : {file_issues_count} erreur(s) d√©tect√©e(s).\n"
                        output_response += "\n".join(file_errors[:3])  # Limiter √† 3 erreurs pour le log
                        file_status = "SUCCESS"  # ‚úÖ CORRIG√â : L'analyse a fonctionn√©
                    
                    # ‚úÖ LOGGING OBLIGATOIRE selon le protocole du TP
                    log_experiment(
                        agent_name="Judge_Agent",
                        model_used=self.model_name,
                        action=ActionType.DEBUG if file_issues_count > 0 else ActionType.ANALYSIS,
                        details={
                            "file_analyzed": file_name,  # ‚úÖ OBLIGATOIRE : Nom du fichier
                            "input_prompt": input_prompt,  # ‚úÖ OBLIGATOIRE : Prompt envoy√©
                            "output_response": output_response,  # ‚úÖ OBLIGATOIRE : R√©ponse
                            "issues_found": file_issues_count,  # ‚úÖ OBLIGATOIRE : Nombre d'erreurs
                            "file_path": file_path,
                            "code_lines": len(code_content.split('\n')) if code_content else 0,
                            "test_directory": target_dir
                        },
                        status=file_status  # ‚úÖ CORRIG√â : SUCCESS m√™me si erreurs d√©tect√©es
                    )
                    
                    print(f"  {'‚úÖ' if file_issues_count == 0 else '‚ùå'} {file_name}: {file_issues_count} erreur(s)")
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur lors de l'analyse de {file_name} : {str(e)}")
                    
                    # Log en cas d'erreur de lecture du fichier
                    log_experiment(
                        agent_name="Judge_Agent",
                        model_used=self.model_name,
                        action=ActionType.DEBUG,
                        details={
                            "file_analyzed": file_name,
                            "input_prompt": f"Tentative d'analyse de {file_name} dans {target_dir}",
                            "output_response": f"‚ùå Erreur lors de la lecture : {str(e)}",
                            "issues_found": 1,
                            "error_type": type(e).__name__
                        },
                        status="FAILURE"  # ‚úÖ Ici FAILURE est correct car erreur syst√®me
                    )
            
            # ‚úÖ √âTAPE 4 : Analyse LLM si des erreurs existent
            if failed > 0:
                print(f"\n‚ùå [JUDGE] {failed} test(s) ont √©chou√© - Analyse LLM en cours...")
                analysis, llm_raw_response = self._analyze_test_failures(errors, target_dir)
                
                # Logger l'analyse LLM globale
                log_experiment(
                    agent_name="Judge_Agent",
                    model_used=self.model_name,
                    action=ActionType.DEBUG,
                    details={
                        "file_analyzed": "global_analysis",
                        "input_prompt": self._build_analysis_prompt(errors),
                        "output_response": llm_raw_response,
                        "issues_found": failed,
                        "recommendations": analysis.get("recommendations", []),
                        "root_causes": analysis.get("root_causes", [])
                    },
                    status="SUCCESS"  # ‚úÖ CORRIG√â : L'analyse a fonctionn√©
                )
                
                return {
                    "success": False,
                    "passed": passed,
                    "failed": failed,
                    "errors": errors,
                    "recommendations": analysis.get("recommendations", []),
                    "root_causes": analysis.get("root_causes", [])
                }
            else:
                print("‚úÖ [JUDGE] Tous les tests passent !")
                return {
                    "success": True,
                    "passed": passed,
                    "failed": 0,
                    "errors": [],
                    "recommendations": []
                }
                
        except Exception as e:
            print(f"‚ùå [JUDGE] Erreur critique lors de l'ex√©cution des tests : {str(e)}")
            
            # ‚úÖ LOGGING de l'erreur critique
            log_experiment(
                agent_name="Judge_Agent",
                model_used=self.model_name,
                action=ActionType.DEBUG,
                details={
                    "file_analyzed": "system_error",
                    "input_prompt": f"Ex√©cution de pytest sur {target_dir}",
                    "output_response": f"‚ùå Erreur syst√®me : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__,
                    "test_directory": target_dir
                },
                status="FAILURE"  # ‚úÖ Ici FAILURE est correct car erreur syst√®me
            )
            
            return {
                "success": False,
                "passed": 0,
                "failed": 0,
                "errors": [str(e)],
                "recommendations": ["V√©rifier que pytest est correctement install√© et que les tests sont valides"]
            }
    
    def _filter_errors_for_file(self, errors: List[str], file_name: str) -> List[str]:
        """
        Filtre les erreurs pour ne garder que celles li√©es √† un fichier sp√©cifique.
        
        Args:
            errors: Liste de toutes les erreurs pytest
            file_name: Nom du fichier √† filtrer (ex: "messy_code.py")
            
        Returns:
            Liste des erreurs concernant ce fichier
        """
        file_errors = []
        file_name_without_ext = file_name.replace('.py', '')
        
        for error in errors:
            # V√©rifier si le nom du fichier appara√Æt dans l'erreur
            if file_name in error or file_name_without_ext in error:
                file_errors.append(error)
        
        return file_errors
    
    def _analyze_test_failures(self, errors: List[str], target_dir: str) -> Tuple[Dict, str]:
        """
        Analyse les √©checs de tests avec le LLM pour identifier les causes profondes.
        
        Args:
            errors: Liste des messages d'erreur
            target_dir: Dossier test√©
            
        Returns:
            Tuple[Dict, str]: (Analyse structur√©e, R√©ponse brute du LLM)
        """
        try:
            analysis_prompt = self._build_analysis_prompt(errors)
            llm_response = self._call_llm(analysis_prompt)
            analysis = self._parse_analysis_response(llm_response)
            return analysis, llm_response
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de l'analyse LLM : {str(e)}")
            return {
                "recommendations": ["Corriger les erreurs de test d√©tect√©es"],
                "root_causes": ["Erreur d'analyse LLM"],
                "severity": "unknown"
            }, f"Erreur : {str(e)}"
    
    def _build_analysis_prompt(self, errors: List[str]) -> str:
        """Construit le prompt pour analyser les erreurs avec le LLM."""
        errors_text = "\n\n".join([
            f"ERREUR {i+1}:\n{error}" 
            for i, error in enumerate(errors[:5])  # Limiter √† 5 erreurs
        ])
        
        return f"""Analyse ces erreurs de tests pytest et identifie les causes profondes.

NOMBRE D'ERREURS : {len(errors)}

MESSAGES D'ERREUR :
{errors_text}

INSTRUCTIONS :
1. Identifie le type d'erreur (AssertionError, TypeError, ImportError, etc.)
2. Localise la ligne probl√©matique
3. Propose des solutions concr√®tes

G√©n√®re un rapport JSON avec les champs suivants :
- recommendations : Liste de solutions
- root_causes : Liste des causes identifi√©es
- severity : "high", "medium" ou "low"

R√©ponds UNIQUEMENT avec du JSON valide."""
    
    def _call_llm(self, prompt: str) -> str:
        """Appelle le LLM avec le system prompt et le user prompt."""
        messages = [
            {"role": "system", "content": JUDGE_SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        response = self.llm.invoke(messages)
        
        # Gestion des diff√©rents formats de r√©ponse
        if isinstance(response.content, list):
            content = response.content[0] if response.content else ""
            if hasattr(content, 'text'):
                return content.text
            return str(content)
        
        return response.content
    
    def _parse_analysis_response(self, response: str) -> Dict:
        """Parse la r√©ponse JSON du LLM."""
        import json
        
        if not isinstance(response, str):
            print(f"‚ö†Ô∏è  R√©ponse LLM inattendue (type: {type(response)})")
            return {
                "recommendations": ["Corriger les erreurs de test"],
                "root_causes": ["Format de r√©ponse LLM inattendu"],
                "severity": "unknown"
            }
        
        try:
            # Nettoyer la r√©ponse des balises markdown
            cleaned = response.strip()
            if cleaned.startswith("```json"):
                cleaned = cleaned[7:]
            if cleaned.startswith("```"):
                cleaned = cleaned[3:]
            if cleaned.endswith("```"):
                cleaned = cleaned[:-3]
            cleaned = cleaned.strip()
            
            # Parser le JSON
            data = json.loads(cleaned)
            
            # Normaliser la structure
            return {
                "recommendations": data.get("recommendations", [data.get("recommendation", "Corriger les erreurs")]),
                "root_causes": [data.get("root_cause", "Cause inconnue")] if isinstance(data.get("root_cause"), str) else data.get("root_causes", ["Cause inconnue"]),
                "severity": data.get("severity", "medium")
            }
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è  Erreur de parsing JSON : {str(e)}")
            return {
                "recommendations": [response[:200]] if response else ["Corriger les erreurs"],
                "root_causes": ["Analyse non structur√©e"],
                "severity": "unknown"
            }