
"""
Swarm Controller
RÃ´le : Orchestrer les 3 agents (Auditor, Fixer, Judge) dans une boucle itÃ©rative
"""

import os
import argparse
from typing import Dict
from dotenv import load_dotenv

load_dotenv()

from src.agents.auditor import AuditorAgent
from src.agents.fixer import FixerAgent
from src.agents.judge import JudgeAgent


from src.utils.logger import log_experiment, ActionType


def run_refactoring_swarm(
    target_dir: str,
    model_name: str = "gemini-2.0-flash-exp",
    max_iterations: int = 10,
    generate_tests: bool = True,
    generate_docs: bool = False
) -> Dict:
    """
    Fonction principale d'orchestration du Swarm.
    
    Workflow :
    1. Auditor analyse le code (ActionType.ANALYSIS)
    2. Fixer corrige selon le rapport (ActionType.FIX)
    3. Fixer gÃ©nÃ¨re les tests manquants (ActionType.GENERATION) [optionnel]
    4. Judge valide avec les tests (ActionType.DEBUG/ANALYSIS)
    5. RÃ©pÃ¨te jusqu'Ã  succÃ¨s ou max_iterations
    6. Fixer gÃ©nÃ¨re la documentation (ActionType.GENERATION) [optionnel]
    
    Args:
        target_dir: Dossier contenant le code Ã  refactoriser
        model_name: Nom du modÃ¨le LLM Ã  utiliser
        max_iterations: Nombre maximum d'itÃ©rations (dÃ©faut: 10)
        generate_tests: GÃ©nÃ©rer automatiquement les tests unitaires manquants
        generate_docs: GÃ©nÃ©rer automatiquement la documentation
    
    Returns:
        Dict: RÃ©sultats finaux avec statut et statistiques
    """
    
    print("="*80)
    print("ğŸš€ DÃ‰MARRAGE DU SWARM DE REFACTORISATION")
    print("="*80)
    print(f"ğŸ“ Dossier cible : {target_dir}")
    print(f"ğŸ¤– ModÃ¨le LLM : {model_name}")
    print(f"ğŸ”„ ItÃ©rations max : {max_iterations}")
    print(f"ğŸ§ª GÃ©nÃ©ration tests : {'âœ…' if generate_tests else 'âŒ'}")
    print(f"ğŸ“ GÃ©nÃ©ration docs : {'âœ…' if generate_docs else 'âŒ'}")
    print("="*80)
    
    # VÃ©rifier que le dossier existe
    if not os.path.exists(target_dir):
        raise FileNotFoundError(f"âŒ Le dossier {target_dir} n'existe pas")
    
    # =========================================================================
    # INITIALISATION DES AGENTS
    # =========================================================================
    print("\nğŸ”§ Initialisation des agents...")
    auditor = AuditorAgent(model_name=model_name)
    fixer = FixerAgent(model_name=model_name)
    judge = JudgeAgent(model_name=model_name)
    print("âœ… Tous les agents sont prÃªts\n")
    
    # Variables de suivi
    iteration = 0
    all_tests_passed = False
    history = []
    
    # =========================================================================
    # BOUCLE PRINCIPALE DE REFACTORISATION
    # =========================================================================
    while iteration < max_iterations and not all_tests_passed:
        iteration += 1
        
        print("\n" + "="*80)
        print(f"ğŸ”„ ITÃ‰RATION {iteration}/{max_iterations}")
        print("="*80)
        
        try:
            # =================================================================
            # Ã‰TAPE 1 : AUDIT (ActionType.ANALYSIS)
            # =================================================================
            print("\nğŸ“‹ Ã‰TAPE 1/4 : Analyse du code par l'Auditor")
            print("-"*80)
            
            audit_report = auditor.analyze(target_dir=target_dir)
            
            print(f"\nâœ… Analyse terminÃ©e :")
            print(f"   ğŸ“ Fichiers analysÃ©s : {len(audit_report['files_analyzed'])}")
            print(f"   âš ï¸  ProblÃ¨mes dÃ©tectÃ©s : {audit_report['total_issues']}")
            
            if audit_report['total_issues'] > 0:
                print(f"\nğŸ’¡ Recommandations :")
                for rec in audit_report.get('recommendations', [])[:3]:
                    print(f"   - {rec}")
            
            # =================================================================
            # Ã‰TAPE 2 : CORRECTION (ActionType.FIX)
            # =================================================================
            print("\nğŸ”§ Ã‰TAPE 2/4 : Correction du code par le Fixer")
            print("-"*80)
            
            if audit_report['total_issues'] == 0:
                print("âœ… Aucun problÃ¨me Ã  corriger, passage direct aux tests")
                fix_result = {
                    "files_fixed": [],
                    "total_fixes": 0,
                    "status": "no_issues"
                }
            else:
                fix_result = fixer.fix(
                    audit_report=audit_report,
                    target_dir=target_dir
                )
                
                print(f"\nâœ… Corrections terminÃ©es :")
                print(f"   ğŸ“ Fichiers corrigÃ©s : {len(fix_result['files_fixed'])}")
                print(f"   ğŸ”§ Corrections appliquÃ©es : {fix_result['total_fixes']}")
                
                if fix_result['files_fixed']:
                    print(f"   ğŸ“„ Fichiers modifiÃ©s : {', '.join(fix_result['files_fixed'])}")
            
            # =================================================================
            # Ã‰TAPE 3 : GÃ‰NÃ‰RATION DE TESTS (ActionType.GENERATION)
            # =================================================================
            if generate_tests and iteration == 1:  # Seulement Ã  la 1Ã¨re itÃ©ration
                print("\nğŸ§ª Ã‰TAPE 3/4 : GÃ©nÃ©ration des tests unitaires manquants")
                print("-"*80)
                
                # Lister les fichiers Python (hors tests)
                python_files = [
                    f for f in os.listdir(target_dir)
                    if f.endswith(".py") and not f.startswith("test_") and f != "__init__.py"
                ]
                
                tests_generated = []
                
                for filename in python_files:
                    test_filename = f"test_{filename}"
                    test_filepath = os.path.join(target_dir, test_filename)
                    
                    # GÃ©nÃ©rer des tests si le fichier de test n'existe pas
                    if not os.path.exists(test_filepath):
                        print(f"   ğŸ§ª GÃ©nÃ©ration de tests pour {filename}...")
                        try:
                            fixer.generate_tests(filename, target_dir)
                            tests_generated.append(test_filename)
                            print(f"      âœ… {test_filename} crÃ©Ã©")
                        except Exception as e:
                            print(f"      âš ï¸  Ã‰chec : {str(e)}")
                    else:
                        print(f"   âœ… Tests dÃ©jÃ  prÃ©sents pour {filename}")
                
                if tests_generated:
                    print(f"\nâœ… {len(tests_generated)} fichier(s) de tests gÃ©nÃ©rÃ©(s)")
                else:
                    print(f"\nâœ… Tous les fichiers ont dÃ©jÃ  leurs tests")
            else:
                print("\nâ­ï¸  Ã‰TAPE 3/4 : GÃ©nÃ©ration de tests (ignorÃ©e)")
            
            # =================================================================
            # Ã‰TAPE 4 : VALIDATION (ActionType.DEBUG ou ANALYSIS)
            # =================================================================
            print("\nğŸ§ª Ã‰TAPE 4/4 : Validation par le Judge")
            print("-"*80)
            
            test_result = judge.test(target_dir=target_dir)
            
            print(f"\nğŸ“Š RÃ©sultats des tests :")
            print(f"   âœ… Tests rÃ©ussis : {test_result['passed']}")
            print(f"   âŒ Tests Ã©chouÃ©s : {test_result['failed']}")
            
            # =================================================================
            # DÃ‰CISION : CONTINUER OU ARRÃŠTER ?
            # =================================================================
            if test_result["success"]:
                all_tests_passed = True
                print("\nğŸ‰ğŸ‰ğŸ‰ SUCCÃˆS ! Tous les tests passent ! ğŸ‰ğŸ‰ğŸ‰")
                print("="*80)
                break
            else:
                print(f"\nâš ï¸  {test_result['failed']} test(s) ont Ã©chouÃ©")
                print("ğŸ”„ Nouvelle itÃ©ration nÃ©cessaire...")
                
                # Afficher les recommandations du Judge
                if test_result.get("recommendations"):
                    print("\nğŸ’¡ Recommandations du Judge :")
                    for rec in test_result["recommendations"][:5]:
                        print(f"   - {rec}")
                
                # Si pas la derniÃ¨re itÃ©ration, essayer de corriger avec retry_fix
                if iteration < max_iterations:
                    print("\nğŸ”„ Tentative de correction avec feedback des tests...")
                    
                    # Identifier les fichiers problÃ©matiques
                    errors = test_result.get("errors", [])
                    python_files = [
                        f for f in os.listdir(target_dir)
                        if f.endswith(".py") and not f.startswith("test_")
                    ]
                    
                    problematic_files = set()
                    for error in errors:
                        for pfile in python_files:
                            if pfile in error or pfile.replace('.py', '') in error:
                                problematic_files.add(pfile)
                    
                    # RÃ©essayer de corriger chaque fichier problÃ©matique
                    for filename in problematic_files:
                        filepath = os.path.join(target_dir, filename)
                        
                        # Filtrer les erreurs pour ce fichier
                        file_errors = [e for e in errors if filename in e]
                        error_message = "\n".join(file_errors[:3])  # Max 3 erreurs
                        
                        print(f"   ğŸ”„ Correction de {filename}...")
                        try:
                            fixer.retry_fix(filepath, target_dir, error_message)
                            print(f"      âœ… Nouvelle version gÃ©nÃ©rÃ©e")
                        except Exception as e:
                            print(f"      âš ï¸  Ã‰chec : {str(e)}")
            
            # Enregistrer l'historique de cette itÃ©ration
            history.append({
                "iteration": iteration,
                "issues_detected": audit_report['total_issues'],
                "fixes_applied": fix_result.get('total_fixes', 0),
                "tests_passed": test_result['passed'],
                "tests_failed": test_result['failed']
            })
            
        except Exception as e:
            print(f"\nâŒ ERREUR lors de l'itÃ©ration {iteration} : {str(e)}")
            
            # Logger l'erreur de l'orchestrateur
            log_experiment(
                agent_name="Swarm_Controller",
                model_used=model_name,
                action=ActionType.DEBUG,
                details={
                    "file_analyzed": f"iteration_{iteration}",
                    "input_prompt": f"Orchestration itÃ©ration {iteration} sur {target_dir}",
                    "output_response": f"Erreur : {str(e)}",
                    "issues_found": 1,
                    "error_type": type(e).__name__,
                    "iteration": iteration
                },
                status="FAILURE"
            )
            
            import traceback
            traceback.print_exc()
            break
    
    # =========================================================================
    # Ã‰TAPE FINALE : GÃ‰NÃ‰RATION DE DOCUMENTATION (ActionType.GENERATION)
    # =========================================================================
    if all_tests_passed and generate_docs:
        print("\n" + "="*80)
        print("ğŸ“ GÃ‰NÃ‰RATION DE LA DOCUMENTATION")
        print("="*80)
        
        python_files = [
            f for f in os.listdir(target_dir)
            if f.endswith(".py") and not f.startswith("test_") and f != "__init__.py"
        ]
        
        for filename in python_files:
            doc_filename = f"README_{filename.replace('.py', '')}.md"
            doc_filepath = os.path.join(target_dir, doc_filename)
            
            if not os.path.exists(doc_filepath):
                print(f"\nğŸ“ GÃ©nÃ©ration de documentation pour {filename}...")
                try:
                    fixer.generate_documentation(filename, target_dir)
                    print(f"   âœ… {doc_filename} crÃ©Ã©")
                except Exception as e:
                    print(f"   âš ï¸  Ã‰chec : {str(e)}")
            else:
                print(f"âœ… Documentation dÃ©jÃ  prÃ©sente pour {filename}")
    
    # =========================================================================
    # RAPPORT FINAL
    # =========================================================================
    print("\n" + "="*80)
    print("ğŸ FIN DU SWARM - RAPPORT FINAL")
    print("="*80)
    
    final_result = {
        "success": all_tests_passed,
        "total_iterations": iteration,
        "max_iterations_reached": iteration >= max_iterations and not all_tests_passed,
        "history": history,
        "target_dir": target_dir,
        "model_used": model_name
    }
    
    if all_tests_passed:
        print(f"\nâœ…âœ…âœ… MISSION ACCOMPLIE en {iteration} itÃ©ration(s) ! âœ…âœ…âœ…")
        print(f"   ğŸ¯ Tous les tests passent avec succÃ¨s")
        if generate_tests:
            print(f"   ğŸ§ª Tests unitaires gÃ©nÃ©rÃ©s")
        if generate_docs:
            print(f"   ğŸ“ Documentation gÃ©nÃ©rÃ©e")
    else:
        print(f"\nâŒ Ã‰CHEC aprÃ¨s {iteration} itÃ©ration(s)")
        if iteration >= max_iterations:
            print(f"   âš ï¸  Nombre maximum d'itÃ©rations atteint ({max_iterations})")
        print(f"   âš ï¸  Certains tests Ã©chouent encore")
    
    print("\nğŸ“Š STATISTIQUES PAR ITÃ‰RATION :")
    print("-"*80)
    for i, iter_data in enumerate(history, 1):
        print(f"   ItÃ©ration {i} :")
        print(f"      ğŸ“‹ ProblÃ¨mes dÃ©tectÃ©s : {iter_data['issues_detected']}")
        print(f"      ğŸ”§ Corrections appliquÃ©es : {iter_data['fixes_applied']}")
        print(f"      âœ… Tests rÃ©ussis : {iter_data['tests_passed']}")
        print(f"      âŒ Tests Ã©chouÃ©s : {iter_data['tests_failed']}")
    
    print("\n" + "="*80)
    
    return final_result


def main():
    """
    Point d'entrÃ©e principal avec gestion des arguments CLI.
    Conforme aux exigences du TP (argument --target_dir obligatoire).
    """
    
    parser = argparse.ArgumentParser(
        description="ğŸš€ The Refactoring Swarm - SystÃ¨me multi-agents de refactorisation automatique",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :
  python main.py --target_dir ./sandbox/messy_code
  python main.py --target_dir ./sandbox/dataset_inconnu --max_iterations 5
  python main.py --target_dir ./sandbox --generate_tests --generate_docs
        """
    )
    
    # Argument OBLIGATOIRE pour le TP
    parser.add_argument(
        "--target_dir",
        type=str,
        required=True,
        help="ğŸ“ Dossier contenant le code Ã  refactoriser (OBLIGATOIRE)"
    )
    
    # Arguments optionnels
    parser.add_argument(
        "--model",
        type=str,
        default="gemini-2.0-flash-exp",
        help="ğŸ¤– Nom du modÃ¨le LLM Ã  utiliser (dÃ©faut: gemini-2.0-flash-exp)"
    )
    
    parser.add_argument(
        "--max_iterations",
        type=int,
        default=10,
        help="ğŸ”„ Nombre maximum d'itÃ©rations (dÃ©faut: 10)"
    )
    
    parser.add_argument(
        "--generate_tests",
        action="store_true",
        help="ğŸ§ª GÃ©nÃ©rer automatiquement les tests unitaires manquants"
    )
    
    parser.add_argument(
        "--generate_docs",
        action="store_true",
        help="ğŸ“ GÃ©nÃ©rer automatiquement la documentation"
    )
    
    parser.add_argument(
        "--no_generation",
        action="store_true",
        help="ğŸš« DÃ©sactiver toute gÃ©nÃ©ration automatique (tests et docs)"
    )
    
    args = parser.parse_args()
    
    # Gestion du flag no_generation
    if args.no_generation:
        generate_tests = False
        generate_docs = False
    else:
        generate_tests = args.generate_tests
        generate_docs = args.generate_docs
    
    try:
        result = run_refactoring_swarm(
            target_dir=args.target_dir,
            model_name=args.model,
            max_iterations=args.max_iterations,
            generate_tests=generate_tests,
            generate_docs=generate_docs
        )
        
        # Code de sortie conforme au TP
        if result["success"]:
            print("\nâœ… Le code a Ã©tÃ© refactorisÃ© avec succÃ¨s !")
            exit(0)
        else:
            print("\nâŒ La refactorisation a Ã©chouÃ©")
            exit(1)
            
    except Exception as e:
        print(f"\nâŒ ERREUR CRITIQUE : {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)


if __name__ == "__main__":
    main()